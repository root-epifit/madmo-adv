{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python [conda env:ml-mipt]",
      "language": "python",
      "name": "conda-env-ml-mipt-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "hw1_boosting_and_explanation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/root-epifit/madmo-adv/blob/my_exercise/homeworks/hw1_boosting_and_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAyFp6Yjj5O-"
      },
      "source": [
        "# ДЗ №1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-86e0de040aac317a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "7kpv5agRjvY-"
      },
      "source": [
        "# Homework 1\n",
        "## Gradient boosting on temporal data and feature importances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUM8bx4PjvZG"
      },
      "source": [
        "Here we will work with widely known Human Actividy Recognition (HAR) dataset. Data is available at [UCI repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones).\n",
        "\n",
        "There are available both raw and preprocessed datasets. This time we will use the preprocessed one.\n",
        "\n",
        "Today we will work with [LightGBM](https://github.com/Microsoft/LightGBM) by Microsoft. It is one of the most popular frameworks these days that shows both great quality and performance.\n",
        "\n",
        "There are another great frameworks (listed below). However, we will stick to `LightGBM` for this task.\n",
        "* [Catboost](https://github.com/catboost/catboost) by Yandex. Novel framework by Yandex company tuned to deal well with categorical features.\n",
        "* [xgboost](https://github.com/dmlc/xgboost) by dlmc. The most famous framework which got very popular on kaggle.\n",
        "\n",
        "Some simple preprocessing is done for you. \n",
        "\n",
        "Your __ultimate target is to get familiar with one of the frameworks above__ and achieve at least 90% accuracy on test dataset and try to get some useful insights on the features the model paid attention to.\n",
        "\n",
        "__Please, use [`shap`](https://github.com/slundberg/shap) and interpret the generated plots (with a brief summary).__\n",
        "\n",
        "_Despite the main language of this notebook is English, feel free to write your thoughts in Russian._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzF1qs4vjvZI"
      },
      "source": [
        "## Part 0. Downloading and preprocessing\n",
        "\n",
        "The preprocessing is done for you. Let's take a look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-01T21:00:39.321460Z",
          "start_time": "2021-11-01T21:00:39.072714Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpy2PfmsjvZK",
        "outputId": "4ee720db-c146-4169-bb36-872e5c8869a0"
      },
      "source": [
        "!wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
        "!unzip -u \"UCI HAR Dataset.zip\" \"UCI HAR Dataset/train/X_train.txt\" \"UCI HAR Dataset/train/y_train.txt\"\\\n",
        "\"UCI HAR Dataset/test/X_test.txt\" \"UCI HAR Dataset/test/y_test.txt\" \"UCI HAR Dataset/activity_labels.txt\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-06 09:24:40--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60999314 (58M) [application/x-httpd-php]\n",
            "Saving to: ‘UCI HAR Dataset.zip’\n",
            "\n",
            "UCI HAR Dataset.zip 100%[===================>]  58.17M  81.1MB/s    in 0.7s    \n",
            "\n",
            "2021-11-06 09:24:41 (81.1 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314]\n",
            "\n",
            "Archive:  UCI HAR Dataset.zip\n",
            "  inflating: UCI HAR Dataset/activity_labels.txt  \n",
            "  inflating: UCI HAR Dataset/test/y_test.txt  \n",
            "  inflating: UCI HAR Dataset/train/X_train.txt  \n",
            "caution: filename not matched:  UCI HAR Dataset/train/y_train.txtUCI HAR Dataset/test/X_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-66iHulqm7Zt",
        "outputId": "eb60a5aa-dea7-48c7-d7dd-e72d00d1debb"
      },
      "source": [
        "!unzip -u \"UCI HAR Dataset.zip\" \"UCI HAR Dataset/train/X_train.txt\" \"UCI HAR Dataset/train/y_train.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  UCI HAR Dataset.zip\n",
            "  inflating: UCI HAR Dataset/train/y_train.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWAS1Qn5neH_",
        "outputId": "772339fb-d751-486d-ad6d-72cbaea3cb77"
      },
      "source": [
        "!unzip -u \"UCI HAR Dataset.zip\" \"UCI HAR Dataset/test/X_test.txt\" \"UCI HAR Dataset/test/y_test.txt\" \"UCI HAR Dataset/activity_labels.txt\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  UCI HAR Dataset.zip\n",
            "  inflating: UCI HAR Dataset/test/X_test.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-01T21:02:20.491291Z",
          "start_time": "2021-11-01T21:02:20.266371Z"
        },
        "id": "eyRhByJfjvZN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-01T21:02:41.967012Z",
          "start_time": "2021-11-01T21:02:38.548196Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVGqO3myjvZO",
        "outputId": "1f51aa54-2cec-4382-e78c-e0fd71ce4918"
      },
      "source": [
        "X_train = np.genfromtxt(\"UCI HAR Dataset/train/X_train.txt\")\n",
        "y_train = np.genfromtxt(\"UCI HAR Dataset/train/y_train.txt\")\n",
        "\n",
        "X_test = np.genfromtxt(\"UCI HAR Dataset/test/X_test.txt\")\n",
        "y_test = np.genfromtxt(\"UCI HAR Dataset/test/y_test.txt\")\n",
        "\n",
        "activity_labels = {}\n",
        "with open(\"UCI HAR Dataset/activity_labels.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "        label, name = line.strip().split(\" \")\n",
        "        activity_labels[int(label)] = name\n",
        "\n",
        "activity_labels"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'WALKING',\n",
              " 2: 'WALKING_UPSTAIRS',\n",
              " 3: 'WALKING_DOWNSTAIRS',\n",
              " 4: 'SITTING',\n",
              " 5: 'STANDING',\n",
              " 6: 'LAYING'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEKYdqB8n-CP",
        "outputId": "88b9fa8c-6009-40d8-f6a9-308ba090f350"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.28858451, -0.02029417, -0.13290514, ..., -0.84124676,\n",
              "         0.17994061, -0.05862692],\n",
              "       [ 0.27841883, -0.01641057, -0.12352019, ..., -0.8447876 ,\n",
              "         0.18028889, -0.05431672],\n",
              "       [ 0.27965306, -0.01946716, -0.11346169, ..., -0.84893347,\n",
              "         0.18063731, -0.04911782],\n",
              "       ...,\n",
              "       [ 0.27338737, -0.01701062, -0.04502183, ..., -0.77913261,\n",
              "         0.24914484,  0.04081119],\n",
              "       [ 0.28965416, -0.01884304, -0.15828059, ..., -0.78518142,\n",
              "         0.24643223,  0.02533948],\n",
              "       [ 0.35150347, -0.01242312, -0.20386717, ..., -0.78326693,\n",
              "         0.24680852,  0.03669484]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-01T20:18:00.103956Z",
          "start_time": "2021-11-01T20:18:00.028869Z"
        },
        "id": "O5y-lxyzjvZP"
      },
      "source": [
        "print(X_train.shape)\n",
        "data_mean = X_train.mean(axis=0)\n",
        "data_std = X_train.std(axis=0)\n",
        "\n",
        "X_train = (X_train - data_mean) / data_std\n",
        "X_test = (X_test - data_mean) / data_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO2e_1AdjvZR"
      },
      "source": [
        "The dataset has some duplicating features. File `unique_columns.txt` stores the indices of the unique ones. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-01T20:18:01.247829Z",
          "start_time": "2021-11-01T20:18:01.109806Z"
        },
        "id": "0izc56PbjvZT"
      },
      "source": [
        "unique_columns = np.genfromtxt(\"unique_columns.txt\", delimiter=\",\").astype(int)\n",
        "X_train_unique = X_train[:, unique_columns]\n",
        "X_test_unique = X_test[:, unique_columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaoQGidPjvZU"
      },
      "source": [
        "PCA could be useful in this case. E.g."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9vfy1VijvZV"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg1twdJtjvZW"
      },
      "source": [
        "pca = PCA(0.99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UQucsfujvZX"
      },
      "source": [
        "X_train_pca = pca.fit_transform(X_train_unique)\n",
        "X_test_pca = pca.transform(X_test_unique)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ell3ji9UjvZY"
      },
      "source": [
        "X_train_pca.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0iZ6vUkjvZY"
      },
      "source": [
        "X_test_pca.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVFGyS6jjvZZ"
      },
      "source": [
        "plt.scatter(X_train_pca[:1000, 0], X_train_pca[:1000, 1], c=y_train[:1000])\n",
        "plt.grid()\n",
        "plt.xlabel(\"Principal component 1\")\n",
        "plt.ylabel(\"Principal component 2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C8pYHvMjvZZ"
      },
      "source": [
        "plt.scatter(X_train_pca[:1000, 3], X_train_pca[:1000, 4], c=y_train[:1000])\n",
        "plt.grid()\n",
        "plt.xlabel(\"Principal component 4\")\n",
        "plt.ylabel(\"Principal component 5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppXRaeqwjvZa"
      },
      "source": [
        "## Part 1. Fit the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQoDF5-ZjvZa"
      },
      "source": [
        "Despite optimal parameters (e.g. for xgboost) can be found on the web, we still want you to approximate them by yourself.\n",
        "\n",
        "In this part just check some hyperparams by hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqpG1iAqjvZb"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Example: https://rpubs.com/burakh/har_xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zEODfj8jvZc"
      },
      "source": [
        "Please, write down your thoughts on the experiment results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMIDDLWcjvZc"
      },
      "source": [
        "## Part 2. Use hyper parameter tuning system\n",
        "\n",
        "Use [optuna](https://optuna.org/), [hyperopt](http://hyperopt.github.io/hyperopt/) or any other zero order optimizer to find optimal hyper param set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MURzqibYjvZc"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6uotalcjvZd"
      },
      "source": [
        "### Part 3. Interpret the model predictions\n",
        "Despite you are free to use any approaches to interpret the model predictions, please use [`shap`](https://github.com/slundberg/shap) to build some plots (e.g. the ones we've seen on week02) and try to interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIdsk17zjvZd"
      },
      "source": [
        "import shap\n",
        "\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rBvMmZIjvZd"
      },
      "source": [
        "_Your thoughts about the plots and model behaviour._"
      ]
    }
  ]
}